{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubernetes in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (32.0.1)\n",
      "Requirement already satisfied: tqdm in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: nbformat in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from kubernetes) (0.10)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from nbformat) (4.24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes) (4.9.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.25.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from requests->kubernetes) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from requests->kubernetes) (3.10)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/arnaud/perso/flink-justin/venv/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%run ../common/common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Nodes should have been already in `cluster.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind delete cluster\n",
      "kind delete cluster\n",
      "Log 0 - 2025-07-09 10:24:53 : Deleting cluster \"kind\" ...\n",
      "Log 0 - 2025-07-09 10:24:58 : Deleted nodes: [\"kind-control-plane\" \"kind-worker\" \"kind-worker2\" \"kind-worker3\"]\n",
      "kind create cluster --config=cluster.yaml\n",
      "kind create cluster --config=cluster.yaml\n",
      "Log 0 - 2025-07-09 10:24:58 : Creating cluster \"kind\" ...\n",
      "Log 0 - 2025-07-09 10:24:58 : • Ensuring node image (kindest/node:v1.33.1) 🖼  ...\n",
      "Log 0 - 2025-07-09 10:24:58 : ✓ Ensuring node image (kindest/node:v1.33.1) 🖼\n",
      "Log 0 - 2025-07-09 10:24:58 : • Preparing nodes 📦 📦 📦 📦   ...\n",
      "Log 0 - 2025-07-09 10:25:04 : ✓ Preparing nodes 📦 📦 📦 📦\n",
      "Log 0 - 2025-07-09 10:25:04 : • Writing configuration 📜  ...\n",
      "Log 0 - 2025-07-09 10:25:04 : ✓ Writing configuration 📜\n",
      "Log 0 - 2025-07-09 10:25:04 : • Starting control-plane 🕹️  ...\n",
      "Log 0 - 2025-07-09 10:25:21 : ✓ Starting control-plane 🕹️\n",
      "Log 0 - 2025-07-09 10:25:21 : • Installing CNI 🔌  ...\n",
      "Log 0 - 2025-07-09 10:25:22 : ✓ Installing CNI 🔌\n",
      "Log 0 - 2025-07-09 10:25:22 : • Installing StorageClass 💾  ...\n",
      "Log 0 - 2025-07-09 10:25:23 : ✓ Installing StorageClass 💾\n",
      "Log 0 - 2025-07-09 10:25:23 : • Joining worker nodes 🚜  ...\n",
      "Log 0 - 2025-07-09 10:25:30 : ✓ Joining worker nodes 🚜\n",
      "Log 0 - 2025-07-09 10:25:31 : Set kubectl context to \"kind-kind\"\n",
      "Log 0 - 2025-07-09 10:25:31 : You can now use your cluster with:\n",
      "Log 0 - 2025-07-09 10:25:31 : \n",
      "Log 0 - 2025-07-09 10:25:31 : kubectl cluster-info --context kind-kind\n",
      "Log 0 - 2025-07-09 10:25:31 : \n",
      "Log 0 - 2025-07-09 10:25:31 : Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂\n",
      "sleep 30\n",
      "sleep 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_command(\"kind delete cluster\", shell=False)\n",
    "run_command(\"kind create cluster --config=cluster.yaml\", shell=False)\n",
    "run_command(\"sleep 30\", shell=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../common/common_modules.sh\n",
      "../common/common_modules.sh\n",
      "Log 0 - 2025-07-09 10:28:24 : clusterrolebinding.rbac.authorization.k8s.io/manager-full unchanged\n",
      "Log 0 - 2025-07-09 10:28:26 : customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:26 : customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:26 : customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : namespace/cert-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : serviceaccount/cert-manager-cainjector unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : serviceaccount/cert-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : serviceaccount/cert-manager-webhook unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged\n",
      "Log 0 - 2025-07-09 10:28:27 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-view unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-edit unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews configured\n",
      "Log 0 - 2025-07-09 10:28:28 : role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : role.rbac.authorization.k8s.io/cert-manager:leaderelection unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection configured\n",
      "Log 0 - 2025-07-09 10:28:28 : rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving configured\n",
      "Log 0 - 2025-07-09 10:28:28 : service/cert-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : service/cert-manager-webhook unchanged\n",
      "Log 0 - 2025-07-09 10:28:28 : deployment.apps/cert-manager-cainjector unchanged\n",
      "Log 0 - 2025-07-09 10:28:29 : deployment.apps/cert-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:29 : deployment.apps/cert-manager-webhook unchanged\n",
      "Log 0 - 2025-07-09 10:28:29 : mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured\n",
      "Log 0 - 2025-07-09 10:28:29 : validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured\n",
      "Log 0 - 2025-07-09 10:28:29 : deployment \"cert-manager-webhook\" successfully rolled out\n",
      "Log 0 - 2025-07-09 10:28:30 : namespace/flink-operator-system unchanged\n",
      "Log 0 - 2025-07-09 10:28:30 : customresourcedefinition.apiextensions.k8s.io/flinkclusters.flinkoperator.k8s.io unchanged\n",
      "Log 0 - 2025-07-09 10:28:30 : serviceaccount/flink-operator-controller-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:30 : role.rbac.authorization.k8s.io/flink-operator-leader-election-role unchanged\n",
      "Log 0 - 2025-07-09 10:28:30 : clusterrole.rbac.authorization.k8s.io/flink-operator-manager-role configured\n",
      "Log 0 - 2025-07-09 10:28:30 : clusterrole.rbac.authorization.k8s.io/flink-operator-metrics-reader unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : clusterrole.rbac.authorization.k8s.io/flink-operator-proxy-role unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : rolebinding.rbac.authorization.k8s.io/flink-operator-leader-election-rolebinding unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-manager-rolebinding unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-proxy-rolebinding unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : service/flink-operator-controller-manager-metrics-service unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : service/flink-operator-webhook-service unchanged\n",
      "Log 0 - 2025-07-09 10:28:31 : deployment.apps/flink-operator-controller-manager unchanged\n",
      "Log 0 - 2025-07-09 10:28:33 : certificate.cert-manager.io/flink-operator-serving-cert unchanged\n",
      "Log 0 - 2025-07-09 10:28:33 : issuer.cert-manager.io/flink-operator-selfsigned-issuer unchanged\n",
      "Log 0 - 2025-07-09 10:28:33 : mutatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-mutating-webhook-configuration configured\n",
      "Log 0 - 2025-07-09 10:28:33 : validatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-validating-webhook-configuration configured\n",
      "Log 0 - 2025-07-09 10:28:33 : Error from server (AlreadyExists): namespaces \"manager\" already exists\n",
      "Log 0 - 2025-07-09 10:28:34 : namespace/local-path-storage unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : serviceaccount/local-path-provisioner-service-account unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : deployment.apps/local-path-provisioner unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : storageclass.storage.k8s.io/local-path unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : configmap/local-path-config unchanged\n",
      "Log 0 - 2025-07-09 10:28:34 : error: the path \"./cm-local-path.yaml\" does not exist\n",
      "Log 0 - 2025-07-09 10:29:04 : \"prometheus-community\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-07-09 10:29:04 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-07-09 10:29:05 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:05 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:05 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:05 : ...Successfully got an update from the \"minio-operator\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:05 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:06 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:06 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-07-09 10:29:07 : Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n",
      "Log 0 - 2025-07-09 10:29:08 : podmonitor.monitoring.coreos.com/flink-pod-monitor unchanged\n",
      "Log 0 - 2025-07-09 10:29:08 : \"grafana\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-07-09 10:29:08 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-07-09 10:29:08 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:08 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:08 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:08 : ...Successfully got an update from the \"minio-operator\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:09 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:09 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:09 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-07-09 10:29:09 : Release \"loki\" does not exist. Installing it now.\n",
      "Log 0 - 2025-07-09 10:29:12 : Error: unable to build kubernetes objects from release manifest: [resource mapping not found for name: \"loki\" namespace: \"\" from \"\": no matches for kind \"PodSecurityPolicy\" in version \"policy/v1beta1\"\n",
      "Log 0 - 2025-07-09 10:29:12 : ensure CRDs are installed first, resource mapping not found for name: \"loki-promtail\" namespace: \"\" from \"\": no matches for kind \"PodSecurityPolicy\" in version \"policy/v1beta1\"\n",
      "Log 0 - 2025-07-09 10:29:12 : ensure CRDs are installed first]\n",
      "Log 0 - 2025-07-09 10:29:22 : \"cloudhut\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-07-09 10:29:22 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-07-09 10:29:22 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:22 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:22 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:22 : ...Successfully got an update from the \"minio-operator\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:23 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:23 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-07-09 10:29:23 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-07-09 10:29:23 : ingress.networking.k8s.io/grafana unchanged\n",
      "Log 0 - 2025-07-09 10:29:24 : ingress.networking.k8s.io/prometheus unchanged\n",
      "Log 0 - 2025-07-09 10:29:24 : ingress.networking.k8s.io/flink unchanged\n",
      "Log 0 - 2025-07-09 10:29:24 : Error from server (AlreadyExists): namespaces \"kafka\" already exists\n",
      "Log 0 - 2025-07-09 10:29:25 : Pulled: registry-1.docker.io/bitnamicharts/kafka:32.3.3\n",
      "Log 0 - 2025-07-09 10:29:25 : Digest: sha256:964fd20a39ca56e552c41e6435fa27c4c40c512a81cc10b547d0145087f88b6e\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_command\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../common/common_modules.sh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/ipykernel_3968264/2823847504.py:22\u001b[39m, in \u001b[36mrun_command\u001b[39m\u001b[34m(command, shell, log)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m processes:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, process \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(processes):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         output = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.decode()\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m process.poll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     24\u001b[39m             processes.remove(process)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_command('../common/common_modules.sh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(manager_node, jobmanager_node, taskmanager_nodes) = get_label_nodes(ip_address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             grafana\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                        Path  Backends\n",
      "  ----                        ----  --------\n",
      "  grafana.127-0-0-1.sslip.io  \n",
      "                              /   prom-grafana:80 (10.244.3.5:3000)\n",
      "Annotations:                  kubernetes.io/ingress.class: nginx\n",
      "                              nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age              From                      Message\n",
      "  ----    ------  ----             ----                      -------\n",
      "  Normal  Sync    4s (x2 over 4s)  nginx-ingress-controller  Scheduled for sync\n",
      "\n",
      "\n",
      "Name:             prometheus\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Ingress Class:    <none>\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host                           Path  Backends\n",
      "  ----                           ----  --------\n",
      "  prometheus.127-0-0-1.sslip.io  \n",
      "                                 /   prom-kube-prometheus-stack-prometheus:9090 (10.244.3.6:9090)\n",
      "Annotations:                     kubernetes.io/ingress.class: nginx\n",
      "                                 nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age              From                      Message\n",
      "  ----    ------  ----             ----                      -------\n",
      "  Normal  Sync    4s (x2 over 4s)  nginx-ingress-controller  Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ing -n manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manager node: 172.18.0.2:30080\n",
      "Access to Minio: http://minio.127-0-0-1.nip.io:30080\n",
      "Access to Grafana: http://grafana.127-0-0-1.nip.io:30080\n",
      "Access to Prometheus: http://prometheus.127-0-0-1:30080\n",
      "Job manager address: 172.18.0.4\n",
      "Task manager addresses: 172.18.0.5\n"
     ]
    }
   ],
   "source": [
    "address = \"127-0-0-1\"\n",
    "print(\"Manager node: {}:30080\".format(manager_node))\n",
    "print(\"Access to Minio: http://minio.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Grafana: http://grafana.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Prometheus: http://prometheus.{}:30080\".format(address))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to Minio: http://172.18.0.4:30900\n",
      "Access to Grafana: http://172.18.0.4:30300\n",
      "Access to Prometheus: http://172.18.0.4:30090\n",
      "Job manager address: 172.18.0.5\n",
      "Task manager addresses: 172.18.0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Access to Minio: http://{}:30900\".format(manager_node))\n",
    "print(\"Access to Grafana: http://{}:30300\".format(manager_node))\n",
    "print(\"Access to Prometheus: http://{}:30090\".format(manager_node))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import [Flink dashboard](https://grafana.com/grafana/dashboards/14911) in Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../minio/bucket.sh\n",
      "../minio/bucket.sh\n",
      "Log 0 - 2025-07-08 16:01:05 : namespace/minio-tenant created\n",
      "Log 0 - 2025-07-08 16:01:05 : secret/storage-configuration created\n",
      "Log 0 - 2025-07-08 16:01:05 : secret/storage-user created\n",
      "Log 0 - 2025-07-08 16:01:05 : tenant.minio.min.io/myminio created\n",
      "Log 0 - 2025-07-08 16:01:55 : Forwarding from 127.0.0.1:9000 -> 9000\n",
      "Log 0 - 2025-07-08 16:01:55 : Forwarding from [::1]:9000 -> 9000\n",
      "Log 0 - 2025-07-08 16:02:05 : Handling connection for 9000\n",
      "Log 0 - 2025-07-08 16:02:05 : mc: <ERROR> Unable to initialize new alias from the provided credentials. Server not initialized yet, please try again.\n",
      "Log 0 - 2025-07-08 16:02:05 : Handling connection for 9000\n",
      "Log 0 - 2025-07-08 16:02:10 : Bucket created successfully `myminio/mybucket`.\n"
     ]
    }
   ],
   "source": [
    "run_command('../minio/bucket.sh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
